import os
import json
import hashlib
from datetime import datetime

DATA_DIR = "data"
NEWS_FILE = f"{DATA_DIR}/news.json"
CACHE_FILE = f"{DATA_DIR}/cache.json"

# =====================
# 1Ô∏è‚É£ LOAD CACHE
# =====================
def load_cache():
    if not os.path.exists(CACHE_FILE):
        return {}
    with open(CACHE_FILE, "r", encoding="utf-8") as f:
        return json.load(f)

# =====================
# 2Ô∏è‚É£ SAVE CACHE
# =====================
def save_cache(cache):
    with open(CACHE_FILE, "w", encoding="utf-8") as f:
        json.dump(cache, f, ensure_ascii=False, indent=2)

# =====================
# 3Ô∏è‚É£ LOAD NEWS (ƒê·ªÇ CH·ªêNG TR√ôNG)
# =====================
def load_existing_news():
    if not os.path.exists(NEWS_FILE):
        return []
    with open(NEWS_FILE, "r", encoding="utf-8") as f:
        return json.load(f)

# =====================
# 4Ô∏è‚É£ T·∫†O KEY T·ª™ URL
# =====================
def url_to_key(url: str) -> str:
    return hashlib.md5(url.encode()).hexdigest()

# =====================
# 5Ô∏è‚É£ MAIN LOGIC
# =====================
def process_article(article, cache, existing_urls):
    """
    article = {
        title, content, url, source, country, category
    }
    """

    # ‚ùå DEDUP: n·∫øu URL ƒë√£ t·ªìn t·∫°i ‚Üí b·ªè
    if article["url"] in existing_urls:
        print("‚è≠Ô∏è Skip duplicate:", article["url"])
        return None

    key = url_to_key(article["url"])

    # ‚úÖ CACHE HIT ‚Üí d√πng l·∫°i
    if key in cache:
        print("‚ôªÔ∏è Use cached summary:", article["url"])
        summary = cache[key]["summary"]
        hot_score = cache[key]["hot_score"]

    # ü§ñ CACHE MISS ‚Üí g·ªçi AI
    else:
        print("ü§ñ Call AI:", article["url"])

        summary = fake_ai_summarize(article["content"])
        hot_score = fake_hot_score(article)

        cache[key] = {
            "url": article["url"],
            "summary": summary,
            "hot_score": hot_score,
            "created_at": datetime.utcnow().isoformat()
        }

    return {
        "title": article["title"],
        "summary": summary,
        "country": article["country"],
        "category": article["category"],
        "source": article["source"],
        "hot_score": hot_score,
        "url": article["url"]
    }

# =====================
# 6Ô∏è‚É£ FAKE AI (ƒê·ªÇ TEST)
# =====================
def fake_ai_summarize(text):
    return text[:200] + "..."

def fake_hot_score(article):
    score = 5
    if article["country"] == "VN":
        score += 2
    if "AI" in article["title"]:
        score += 2
    return score

# =====================
# 7Ô∏è‚É£ RUN
# =====================
def run_pipeline(new_articles):
    cache = load_cache()
    news = load_existing_news()

    existing_urls = {n["url"] for n in news}
    new_items = []

    for a in new_articles:
        item = process_article(a, cache, existing_urls)
        if item:
            new_items.append(item)

    if new_items:
        news = new_items + news
        with open(NEWS_FILE, "w", encoding="utf-8") as f:
            json.dump(news, f, ensure_ascii=False, indent=2)

        save_cache(cache)

    print(f"‚úÖ Added {len(new_items)} new items")
if __name__ == "__main__":
    test_articles = [
        {
            "title": "AI Vi·ªát Nam th·ª≠ nghi·ªám drone giao h√†ng",
            "content": "TP HCM th·ª≠ nghi·ªám giao h√†ng b·∫±ng thi·∫øt b·ªã bay kh√¥ng ng∆∞·ªùi l√°i...",
            "url": "https://vnexpress.net/ai-drone",
            "source": "VNExpress",
            "country": "VN",
            "category": "·ª®ng d·ª•ng AI"
        },
        {
            "title": "AI Vi·ªát Nam th·ª≠ nghi·ªám drone giao h√†ng (L·∫∂P)",
            "content": "TP HCM th·ª≠ nghi·ªám giao h√†ng b·∫±ng thi·∫øt b·ªã bay kh√¥ng ng∆∞·ªùi l√°i...",
            "url": "https://vnexpress.net/ai-drone",
            "source": "VNExpress",
            "country": "VN",
            "category": "·ª®ng d·ª•ng AI"
        }
    ]

    run_pipeline(test_articles)

